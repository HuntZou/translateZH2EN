{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f39fa75",
   "metadata": {},
   "source": [
    "## 说明\n",
    "\n",
    "这是一个基于seq2seq的，将中文句子翻译成英文的Encoder-Decoder模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9185c70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchtext.legacy.data import Field, TabularDataset, Iterator, Example\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f585f34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用tensorboard 生成可视化界面\n",
    "# 会在项目目录下生成 runs/ 文件夹, 使用命令 tensorboard --logdir=./runs/ 即可查看图表\n",
    "\n",
    "summaryWriter = SummaryWriter()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be7e983b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "11.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255f37f9",
   "metadata": {},
   "source": [
    "## 数据集\n",
    "使用的是paws-x数据集\n",
    "\n",
    "https://github.com/google-research-datasets/paws/tree/master/pawsx\n",
    "\n",
    "tsv使用 \\t 作为分隔符，csv使用的是逗号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34826bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注：由于双引号在csv中有特殊含义，但语料数据中有些又有双引号，故需加上 quoting=csv.QUOTE_NONE 参数\n",
    "\n",
    "# 训练数据集\n",
    "if not os.path.exists(r'x-final\\translated_zh2en_train.tsv'):\n",
    "    src_zh_train = pd.read_csv(r'x-final\\zh\\translated_train.tsv', delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    trg_en_train = pd.read_csv(r'x-final\\en\\train.tsv', delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "    # 将中文和英文的句子放在一个文件中便于后续处理\n",
    "    src_zh_train[\"sentence3\"] = trg_en_train['sentence1']\n",
    "    src_zh_train[\"sentence4\"] = trg_en_train['sentence2']\n",
    "    src_zh_train = src_zh_train.dropna(how='any')[:49100]\n",
    "    src_zh_train.to_csv(r'x-final\\translated_zh2en_train.tsv', sep='\\t', encoding='utf-8', index=False)\n",
    "\n",
    "# 测试数据集\n",
    "if not os.path.exists(r'x-final\\translated_zh2en_test.tsv'):\n",
    "    src_zh_test = pd.read_csv(r'x-final\\zh\\test_2k.tsv', delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    trg_en_test = pd.read_csv(r'x-final\\en\\test_2k.tsv', delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    src_zh_test[\"sentence3\"] = trg_en_test['sentence1']\n",
    "    src_zh_test[\"sentence4\"] = trg_en_test['sentence2']\n",
    "    src_zh_test = src_zh_test.dropna(how='any')\n",
    "    src_zh_test.to_csv(r'x-final\\translated_zh2en_test.tsv', sep='\\t', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146eec98",
   "metadata": {},
   "source": [
    "### 分词器\n",
    "例如 “我今天中午吃包子”，调用分词器就可以拆分得到 ['我','今天','中午','吃','包子']\n",
    "\n",
    "对于英语来说，就直接是按空格分单词即可\n",
    "\n",
    "为什么要分词？\n",
    "\n",
    "因为对于一个句子来说，它是由词语而并非一个一个的汉字组成，例如上面例子中，若以汉字拆分就会将“天”这个字单独拆分出来，但事实上它必须和“今”组合在一起才能表示它在句中真正的意思"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35ce943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词器\n",
    "spacy_zh = spacy.load('zh_core_web_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "def tokenize_zh(text):\n",
    "    tokens = list(map(lambda doc: doc.text, spacy_zh.tokenizer(text)))\n",
    "    return tokens\n",
    "\n",
    "def tokenize_en(text):\n",
    "    tokens = list(map(lambda doc: doc.text, spacy_en.tokenizer(text)))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2eebfc",
   "metadata": {},
   "source": [
    "## 构建数据集对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fe42606",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize=tokenize_zh, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "fields = [('id', None), ('sentence1', SRC), ('sentence2', None), ('label', None), ('sentence3', TRG), ('sentence4', None)]\n",
    "train_dataset = TabularDataset(path=r'x-final\\translated_zh2en_train.tsv', format='TSV',fields=fields, skip_header=True)\n",
    "test_dataset = TabularDataset(path=r'x-final\\translated_zh2en_test.tsv', format='TSV',fields=fields, skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c929e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11348438",
   "metadata": {},
   "source": [
    "### 构建词汇表\n",
    "\n",
    "即将原本的单词或词语使用一个int的整数代替，便于后续处理\n",
    "\n",
    "`min_freq = 2` 的意思是：只构建出现次数大于等于2的词汇，只出现一次的均使用`<unk>`(unknown)代替\n",
    "\n",
    "构建之后，Field就有vocab属性了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a17dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_dataset, min_freq = 2)\n",
    "TRG.build_vocab(train_dataset, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eecd3480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src vocab len: 36673\n",
      "trg vocab len: 28660\n"
     ]
    }
   ],
   "source": [
    "print(f'src vocab len: {len(SRC.vocab)}')\n",
    "print(f'trg vocab len: {len(TRG.vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b82580",
   "metadata": {},
   "source": [
    "## 构建模型\n",
    "seq2seq模型，也是encoding-decoding 模型\n",
    "\n",
    "### Encoder\n",
    "Encoder的输入是整个句子，他会自动完成时序训练。最终输出一个向量（lstm是两个，但可以将它们合起来看成一个），该向量就可以看成是对整个句子的抽象，即：它里面包含了`整个`句子的特征信息\n",
    "\n",
    "### Decoder\n",
    "Decoder首先需要Encoder的输出作为一个输入（lstm是两个），另外还需要将每次上次时序的output作为下一次的一个输入\n",
    "\n",
    "### Embedding\n",
    "embedding层将原本的一个单词（事实上是单词的一个int类型的索引）转换为一个向量。\n",
    "\n",
    "为什么要这样做？\n",
    "\n",
    "一个单词或一个词语，例如“番茄”和“西红柿”，它俩的索引值肯定完全不同，但是可以通过训练得到一个相同的向量用以说明它们是同一个东西\n",
    "\n",
    "nn.Embedding()的第一个参数是有多少单词需要转换，第二个参数是每个单词转换成多少维的向量\n",
    "\n",
    "### Seq2Seq\n",
    "需要说明的是：\n",
    "1. 对于Encoder是直接将整个完整的句子（即拆分后的词组）传进去的，它自己内部会一个一个词地送给LSTM模型。但对于Decoder来说，我们就需要手动将输入一个一个遍历送进LSTM，因为Decoder的input是上一个时序的output\n",
    "2. 上面说的“Decoder的input是上一个时序的output”，但实际训练过程中，会按照一定概率将“上一个时序的output”替换成ground truth，即真实的值（上一个时序的output是预测值）。这种做法叫做 `teacher force`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a4d27e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.embedding = nn.Embedding(num_embeddings=self.input_dim, embedding_dim=256)\n",
    "        self.lstm = nn.LSTM(input_size=256, hidden_size=512, num_layers=1)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, (h_n, c_n) = self.lstm(x)\n",
    "        return h_n, c_n\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding = nn.Embedding(num_embeddings=self.output_dim, embedding_dim=256)\n",
    "        self.lstm = nn.LSTM(input_size=256, hidden_size=512, num_layers=1)\n",
    "        self.fc = nn.Linear(512, self.output_dim)\n",
    "    def forward(self, x, h_n, c_n):\n",
    "        # 对于Decoder来说，它的输入只是一个一个的单词（单个元素）而并非一整个句子（数组），故需要在外部增加一个维度，表示它是一个长度为1的句子\n",
    "        x = x.unsqueeze(0)\n",
    "        x = self.embedding(x)\n",
    "        output, (h_n, c_n) = self.lstm(x, (h_n, c_n))\n",
    "        \n",
    "        output = output.squeeze(0)\n",
    "        output = self.fc(output)\n",
    "        return output, (h_n, c_n)\n",
    "    \n",
    "class TranslateModule(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self, x, y):\n",
    "        batch_size = y.shape[1]\n",
    "        output_dim = self.decoder.output_dim\n",
    "        \n",
    "        res = torch.zeros(len(y), batch_size, output_dim).to(device) # some times mem out, depend on len(y) & batch_size\n",
    "        \n",
    "        h_n, c_n = self.encoder(x)\n",
    "        next_word = y[0, :]\n",
    "        for i in range(1, len(y)):\n",
    "            output, (h_n, c_n) = self.decoder(next_word, h_n, c_n)\n",
    "            res[i] = output\n",
    "            next_word = output.argmax(1)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eb6311",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ce287c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "\n",
    "epoch = 5\n",
    "# 11*3*3*499 = 49401\n",
    "# 73*673 = 49129\n",
    "batch_size = 50\n",
    "\n",
    "encoder_model = Encoder(INPUT_DIM)\n",
    "decoder_model = Decoder(OUTPUT_DIM)\n",
    "model = TranslateModule(encoder_model, decoder_model).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e97b98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start at Sat Jul 16 11:10:07 2022\n",
      "epoch: 0\n",
      "Sat Jul 16 11:10:12 2022 1 of 982, cuda_mem: 737880064, max_cuda_mem: 921824768, reserved: 1151336448\n",
      "Sat Jul 16 11:16:34 2022 99 of 982, cuda_mem: 737881600, max_cuda_mem: 1490021376, reserved: 2881486848\n",
      "Sat Jul 16 11:22:52 2022 197 of 982, cuda_mem: 743612416, max_cuda_mem: 1490021376, reserved: 2881486848\n",
      "Sat Jul 16 11:29:06 2022 295 of 982, cuda_mem: 743614464, max_cuda_mem: 1490021376, reserved: 2881486848\n",
      "Sat Jul 16 11:35:02 2022 393 of 982, cuda_mem: 755079168, max_cuda_mem: 1490021376, reserved: 2881486848\n",
      "Sat Jul 16 11:40:33 2022 491 of 982, cuda_mem: 749347328, max_cuda_mem: 1490021376, reserved: 2881486848\n",
      "Sat Jul 16 11:46:00 2022 589 of 982, cuda_mem: 743611904, max_cuda_mem: 1490021376, reserved: 2881486848\n",
      "Sat Jul 16 11:51:31 2022 687 of 982, cuda_mem: 755078144, max_cuda_mem: 1490021376, reserved: 2881486848\n",
      "Sat Jul 16 11:57:04 2022 785 of 982, cuda_mem: 737880064, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 12:02:33 2022 883 of 982, cuda_mem: 743614464, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 12:08:06 2022 981 of 982, cuda_mem: 737881088, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "epoch: 1\n",
      "Sat Jul 16 12:08:35 2022 983 of 982, cuda_mem: 737880064, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 12:14:08 2022 1081 of 982, cuda_mem: 737881600, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 12:19:36 2022 1179 of 982, cuda_mem: 743612416, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 12:25:06 2022 1277 of 982, cuda_mem: 743614464, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 12:30:36 2022 1375 of 982, cuda_mem: 755079168, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 12:36:07 2022 1473 of 982, cuda_mem: 749347328, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 12:41:34 2022 1571 of 982, cuda_mem: 743611904, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 12:47:05 2022 1669 of 982, cuda_mem: 755784704, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 12:52:37 2022 1767 of 982, cuda_mem: 737880064, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 12:58:07 2022 1865 of 982, cuda_mem: 743614464, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 13:03:39 2022 1963 of 982, cuda_mem: 737881088, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "epoch: 2\n",
      "Sat Jul 16 13:04:09 2022 1965 of 982, cuda_mem: 737880064, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 13:09:41 2022 2063 of 982, cuda_mem: 737881600, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 13:15:10 2022 2161 of 982, cuda_mem: 743612416, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 13:20:40 2022 2259 of 982, cuda_mem: 743614464, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 13:26:10 2022 2357 of 982, cuda_mem: 755079168, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 13:31:41 2022 2455 of 982, cuda_mem: 749347328, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 13:37:09 2022 2553 of 982, cuda_mem: 743611904, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 13:42:39 2022 2651 of 982, cuda_mem: 755784704, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 13:48:11 2022 2749 of 982, cuda_mem: 737880064, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 13:53:40 2022 2847 of 982, cuda_mem: 743614464, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 13:59:13 2022 2945 of 982, cuda_mem: 737881088, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "epoch: 3\n",
      "Sat Jul 16 13:59:43 2022 2947 of 982, cuda_mem: 737880064, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 14:05:15 2022 3045 of 982, cuda_mem: 737881600, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 14:10:44 2022 3143 of 982, cuda_mem: 743612416, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 14:16:14 2022 3241 of 982, cuda_mem: 743614464, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 14:21:44 2022 3339 of 982, cuda_mem: 755079168, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 14:27:15 2022 3437 of 982, cuda_mem: 749347328, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 14:32:42 2022 3535 of 982, cuda_mem: 743611904, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 14:38:13 2022 3633 of 982, cuda_mem: 755784704, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 14:43:45 2022 3731 of 982, cuda_mem: 737880064, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 14:49:14 2022 3829 of 982, cuda_mem: 743614464, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 14:54:47 2022 3927 of 982, cuda_mem: 737881088, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "epoch: 4\n",
      "Sat Jul 16 14:55:17 2022 3929 of 982, cuda_mem: 737880064, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 15:00:49 2022 4027 of 982, cuda_mem: 737881600, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 15:06:17 2022 4125 of 982, cuda_mem: 743612416, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 15:11:47 2022 4223 of 982, cuda_mem: 743614464, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 15:17:17 2022 4321 of 982, cuda_mem: 755079168, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 15:22:48 2022 4419 of 982, cuda_mem: 749347328, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 15:28:16 2022 4517 of 982, cuda_mem: 743611904, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 15:33:46 2022 4615 of 982, cuda_mem: 755784704, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 15:39:19 2022 4713 of 982, cuda_mem: 737880064, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 15:44:48 2022 4811 of 982, cuda_mem: 743614464, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Sat Jul 16 15:50:21 2022 4909 of 982, cuda_mem: 737881088, max_cuda_mem: 1510753280, reserved: 2074083328\n",
      "Done at Sat Jul 16 15:50:48 2022\n"
     ]
    }
   ],
   "source": [
    "count_train = 0\n",
    "print(f'Start at {time.ctime()}')\n",
    "for e in range(epoch):\n",
    "    print(f'epoch: {e}')\n",
    "    # 训练\n",
    "    model.train()\n",
    "    for idx, i in enumerate(Iterator(dataset=train_dataset, batch_size=batch_size)):\n",
    "        count_train+=1\n",
    "        optimizer.zero_grad()\n",
    "        x = i.sentence1.to(device)\n",
    "        y = i.sentence3.to(device)\n",
    "        pred = model(x, y)\n",
    "        loss = loss_fn(pred.permute(1,2,0), y.permute(1,0))\n",
    "        summaryWriter.add_scalar(r'Loss/train', loss.item(), count_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if idx % int((len(train_dataset)/batch_size)/10) == 0:\n",
    "            print(f'{time.ctime()} {count_train} of {int(len(train_dataset)/batch_size)}, cuda_mem: {torch.cuda.memory_allocated()}, max_cuda_mem: {torch.cuda.max_memory_allocated()}, reserved: {torch.cuda.memory_reserved()}')\n",
    "    # 测试\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_sum = 0\n",
    "        for idx, i in enumerate(Iterator(dataset=test_dataset, batch_size=batch_size)):\n",
    "            x = i.sentence1.to(device)\n",
    "            y = i.sentence3.to(device)\n",
    "            pred = model(x, y)\n",
    "            loss = loss_fn(pred.permute(1,2,0), y.permute(1,0))\n",
    "            loss_sum += loss.item()\n",
    "        summaryWriter.add_scalar(r'Loss/test', loss_sum/int((len(test_dataset)/batch_size)), e)\n",
    "\n",
    "torch.save(model.state_dict(), 'translate.pth')\n",
    "print(f'Done at {time.ctime()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0c7de9",
   "metadata": {},
   "source": [
    "## 模型的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e71b663",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def translate(sentence, max_len):\n",
    "    model.eval()\n",
    "    \n",
    "    # 分词并数字化\n",
    "    sentence = torch.tensor(list(map(lambda w: SRC.vocab.stoi[w], tokenize_zh(sentence))))\n",
    "    \n",
    "    # 翻译\n",
    "    sentence = sentence.unsqueeze(1).to(device)\n",
    "    \n",
    "    # 编码\n",
    "    h_n, c_n = model.encoder(sentence)\n",
    "    \n",
    "    # 解码\n",
    "    target = \"\"\n",
    "    next_word = torch.tensor([SRC.vocab.stoi[SRC.init_token]]).to(device)\n",
    "    for i in range(max_len):\n",
    "        output, (h_n, c_n) = model.decoder(next_word, h_n, c_n)\n",
    "        next_word = output.argmax(1)\n",
    "        real_word = TRG.vocab.itos[next_word[0] if next_word[0] < OUTPUT_DIM else 0]\n",
    "        target += (' ' + real_word)\n",
    "        if real_word == TRG.eos_token:\n",
    "            break\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17f936ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' he married with in the , , with , , , , , \" \" \" . . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('Bouchier与Dorothy Britton结婚，后者将一些日本书籍翻译成英文。', 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fcbba4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
